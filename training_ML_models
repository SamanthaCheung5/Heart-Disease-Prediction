import pandas as pd
import numpy as np
from sklearn.model_selection import KFold, train_test_split, cross_validate
from sklearn.neighbors import KNeighborsClassifier
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, precision_score, recall_score

def read_file(filename):
    '''given the name of a csv file (string),
        read in the contents and return as a 2d list
        of strings '''
    data = pd.read_csv(filename)
    print(data.columns)
    return data

def cross_fold_validation(data):
    ''' given a data frame, performs cross fold validation to 
    find the optimal k value based on accuracy and returns a list 
    containing the mean scores, the best k values for 
    the different scoring metrics and some information about the highest 
    and lowest mean scores 
    '''
    k_values = range(4, 13)
    mean_scores = {'accuracy':[], 'precision':[], 'recall':[]}
    X = data.drop(["Sex", "Heart Disease"], axis=1)
    y = data["Heart Disease"]
    

    for k in k_values:
        model = KNeighborsClassifier(n_neighbors=k)
      
        kf = KFold(n_splits=5, shuffle=True, random_state=42)
        cross = cross_validate(model, X, y, cv=kf, 
                scoring=['accuracy', 'precision_macro', 'recall_macro'])

        mean_accuracy = np.mean(cross['test_accuracy'])
        mean_precision = np.mean(cross['test_precision_macro'])
        mean_recall = np.mean(cross['test_recall_macro'])
    
        mean_scores['accuracy'].append(mean_accuracy)
        mean_scores['precision'].append(mean_precision)
        mean_scores['recall'].append(mean_recall)

    best_accuracy_k = np.argmax(mean_scores['accuracy']) + 4
    best_precision_k = np.argmax(mean_scores['precision']) 
    best_recall_k = np.argmax(mean_scores['recall']) + 4
    
    best_k_values = [best_accuracy_k, best_precision_k, best_recall_k, 
                     mean_scores]
    return best_k_values

def classifer(data, best_k_values):
    '''given a data frame and list containing best_k_values, label maps the y 
    values and splits the X and y into training data, builds a knn
    classifier for the different scoring metrics and returns the 
    scores of each model along with the precision classifer for the user
    model
    '''
    X = data.drop(["Sex", "Heart Disease"], axis=1)
    label_mapping = {'Absence': 0, 'Presence': 1}
    y = data["Heart Disease"].map(label_mapping)
    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)
    accuracy_k, precision_k, recall_k, mean_scores = best_k_values
    
    # Classifier for accuracy
    classifier_accuracy = KNeighborsClassifier(n_neighbors=accuracy_k)
    classifier_accuracy.fit(X_train, y_train)
    y_pred_accuracy = classifier_accuracy.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred_accuracy)
    
    # Classifier for precision
    classifier_precision = KNeighborsClassifier(n_neighbors=precision_k)
    classifier_precision.fit(X_train, y_train)
    y_pred_precision = classifier_precision.predict(X_test)
    precision = precision_score(y_test, y_pred_precision)
    
    # Classifier for recall
    classifier_recall = KNeighborsClassifier(n_neighbors=recall_k)
    classifier_recall.fit(X_train, y_train)
    y_pred_recall = classifier_recall.predict(X_test)
    recall = recall_score(y_test, y_pred_recall)
    
    knn_scores = [accuracy, precision, recall, classifier_precision]
    return knn_scores
    

def logistic_regression(data):
    '''given a data frame, label maps the y values and splits the X and y into 
    training data, builds a logistic regression model and returns the accuracy,
    precision, and recall scores along with the model for the user model
    '''
    X = data.drop(["Sex", "Heart Disease"], axis=1)
    label_mapping = {'Absence': 0, 'Presence': 1}
    y = data["Heart Disease"].map(label_mapping)
    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)
    model = LogisticRegression()
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred)
    recall = recall_score(y_test, y_pred)

    lr_scores = [accuracy, precision, recall, model]
    return lr_scores
    

def plot_k_values(best_k_values):
    '''given a list called best_k_values containing different k values and 
    their mean scores, creates a plot to show how the mean scores change when 
    using different k values and different scoring metrics. this all goes to 
    show why optimal values of k were picked.
    '''
    mean_scores = best_k_values[3]
    accuracy_scores = mean_scores['accuracy']
    precision_scores = mean_scores['precision']
    recall_scores = mean_scores['recall']
    k_values = range(4, 13)
    plt.ylabel("Score")
    plt.xlabel("K-Value")
    plt.title("K Values and Mean Scores for Different Scoring Metrics")
    plt.plot(k_values, accuracy_scores, label="Accuracy")
    plt.plot(k_values, precision_scores, label='Precision')
    plt.plot(k_values, recall_scores, label='Recall')
    plt.legend()

def compare_models(knn_scores, lr_scores):
    '''given a list of the knn scores and the logistic regression scores, 
    plots the scores of each model based on accuracy, precision, and recall
    in a seaborn barplot
    '''
    models = ["K-Nearest Neighbors", "Logistic Regression"]
    metrics = ["Accuracy", "Precision", "Recall"]
    
    # Extracting values from knn_scores and lr_scores
    knn_accuracy, knn_precision, knn_recall, classifer_precision = knn_scores
    lr_accuracy, lr_precision, lr_recall, model= lr_scores
    
       
    data = {
    "Model": models * 3,
    "Metric": metrics * 2,
    "Score": [knn_accuracy, knn_precision, knn_recall, lr_accuracy,
              lr_precision, lr_recall]
    }
    df = pd.DataFrame(data)
    
    # Plotting using seaborn barplot
    plt.figure(figsize=(10, 6))
    ax = sns.barplot(x="Metric", y="Score", hue="Model", data=df, 
                     palette="Set2")
    plt.title('Comparison of Metrics between kNN and Logistic Regression')
    plt.ylabel('Score')
    for p in ax.patches:
        ax.annotate(format(p.get_height(), '.2f'), 
                    (p.get_x() + p.get_width() / 2., p.get_height()), 
                    ha = 'center', va = 'center', 
                    xytext = (0, 10), 
                    textcoords = 'offset points')
    plt.legend(loc='upper right', fontsize='small')

    plt.show()
    
